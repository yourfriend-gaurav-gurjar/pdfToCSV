{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import cycle\n",
    "import pdfplumber as pp\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import os\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import awswrangler as wr\n",
    "\n",
    "# Initialize job parameters\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Assign bucket names.\n",
    "source_bucket = 'cannaeyeglass-datalake-processed-dev'\n",
    "dest_bucket = 'cannaeyeglass-datalake-enriched-dev'\n",
    "#error_bucket = 'cannaeyeglass-datalake-errors'\n",
    "\n",
    "def remove_newline(x):\n",
    "    try:\n",
    "        x = x.str.replace('\\n','').replace(' \\n','').replace('\\n ','')\n",
    "    except:\n",
    "        pass\n",
    "    return x\n",
    "\n",
    "def csvToCombinedcsv(dest_bucket, destination_file, source_bucket, file_path,license_type):\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "    raw_df = wr.s3.read_csv(path=f's3://{source_bucket}/{file_path}/', path_suffix = ['.csv'], dataset=True)\n",
    "    raw_df =raw_df.drop(['Unnamed: 0'],axis=1)\n",
    "    raw_df=raw_df[raw_df['0'].notnull()]\n",
    "    enrich_df= raw_df.iloc[:,:7]\n",
    "    enrich_df.columns=['Name','Lincence_number','Email','Phone','City','Zip','County']\n",
    "    enrich_df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True )\n",
    "    #enrich_df.apply(remove_newline)\n",
    "    enrich_df['license_type']=license_type\n",
    "    with s3.open(f's3://{dest_bucket}/{destination_file}/','w') as f:\n",
    "        enrich_df.to_csv(f)\n",
    "# Calling the function with latest available files\n",
    "date = (datetime.today() - timedelta(0)).strftime('%Y%m%d')\n",
    "year = (datetime.today() - timedelta(0)).strftime('%Y-%m-%d').split('-')[0]\n",
    "month = (datetime.today() - timedelta(0)).strftime('%Y-%m-%d').split('-')[1]\n",
    "day=(datetime.today() - timedelta(0)).strftime('%Y-%m-%d').split('-')[2]\n",
    "filenames = ['growers','processor','transporter','dispensaries','laboratory','waste_disposal']\n",
    "#filenames = ['waste_disposal']\n",
    "for file in filenames:\n",
    "    raw_path_dir = 'US/OK/CannabisLifecycle/' + file + '/'  + year + '/' + month + '/' + day\n",
    "    dest_filename = 'US/OK/LicenceInfo/' + year + '/' + month + '/' + day + '/' + file +'_'+ date\n",
    "    combained='US/OK/LicenseInfo/' + year + '/' + month + '/' + day + '/' + file + date+'.csv'\n",
    "    csvToCombinedcsv(dest_bucket, combained, source_bucket, raw_path_dir,file)\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}